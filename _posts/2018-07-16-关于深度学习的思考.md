---
layout:     post
title:      关于深度学习的思考
subtitle:   About Deep Learning
date:       2018-07-16
catalog: true
tags:
    - Deep Learning

---


> Last updated on 2018-7-20...  

4 月 15 日举办的京东人工智能创新峰会上，周志华教授做了《关于深度学习一点思考》的公开分享，个人写此本篇作为笔记：

### 深度学习

深度神经网络取得成功的三大原因：

- 逐层的处理        （决策树`✔` Boosting`✔`）
- 特征的内部变化    （决策树`✖` Boosting`✖`）
- 足够的模型复杂度  （决策树`✖` Boosting`✖`）

并得出结论：如果满足这三大条件，则并不一定只能用深度神经网络。

深度神经网络缺点如下：

- 容易 overfit
- 训练时候需要很多 trick，而且经验不通用
- 计算开销非常大

今天我们有了深度学习之后，现在不再需要手工设计特征，把数据从一端扔进去，模型从另外一端出来，中间所有的特征完全通过学习自己来解决，这是所谓的`特征学习`或者`表示学习`，这和以往的机器学习技术相比是一个很大的进步，我们不再需要完全依赖人类专家去设计特征了。

其实这一件事情要分两个方面来看：一个方面当我们把特征学习和分类器学习联合起来考虑，可以达到`联合优化`的作用，这是好的方面；但另一方面，如果这里面发生什么我们不清楚，这时候`端到端`的学习不一定真的好，因为可能第一部分往东，第二部分往西，合起来看往东走的更多一些，其实内部有一些东西已经抵消了。

###  为何取深不取宽

-  当你变宽的时候你只不过增加了一些计算单元、增加了函数的个数
-  而在变深的时候不仅增加了个数，其实还增加了嵌入的层次，所以泛函的表达能力会更强

###  Universal approximation theorem

单隐层神经网络即可任意逼近闭集上的任意连续函数，只要隐层节点数量足够多

![](/img/post/20180716/1.png)

###  关于应用

- 如果我们真的仔细去关注，真的神经网络获胜的往往就是在图像、视频、声音这几类典型任务上
- 而在其它涉及到混合建模、离散建模、符号建模的任务上，其实神经网络的性能比其它模型还要差一些

###  展望

以前在神经网络里面我们经常用 Sigmoid，它是`连续可微`的，现在在深度神经网络里，我们经常用 tanh 或者 tanh 的变体，它也是连续可微的。有了这么一个性质以后，我们会得到一个非常好的结果，这个结果就是现在我们可以很容易计算系统的梯度。因此就可以很容易用著名的`BP 算法`来训练这系统。

今天我们谈到的深度模型基本上都是深度神经网络。如果用术语来说的话，它是多层可参数化的可微分的非线性模块所组成的模型，而这个模型可以用 BP 算法来训练。

那么这里面有两个问题：第一，我们现实世界遇到的各种各样的问题的性质，并不是绝对都是可微的，或者能够用可微的模型做最佳建模；第二，过去几十年里面，我们的机器学习界做了很多很多模型出来，这些都可以作为我们构建一个系统的基石，而中间有相当一部分模块是不可微的。

所以我们现在有一个很大的挑战，这不光是学术上也是技术上的挑战，就是我们能不能用不可微的模块来构建深度模型。

### 主要参考文章

  [文章链接](https://zhuanlan.zhihu.com/p/35728514?utm_source=qq&utm_medium=social&utm_oi=566394839504048128)
